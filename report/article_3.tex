%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Stylish Article
% LaTeX Template
% Version 2.1 (1/10/15)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Mathias Legrand (legrand.mathias@gmail.com) 
% With extensive modifications by:
% Vel (vel@latextemplates.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[fleqn,10pt]{SelfArx} % Document font size and equations flushed left
\usepackage{fontspec}
\usepackage[english]{babel} % Specify a different language here - english by default

\usepackage{lipsum} % Required to insert dummy text. To be removed otherwise
\usepackage{float}
%----------------------------------------------------------------------------------------
%	COLUMNS
%----------------------------------------------------------------------------------------

\setlength{\columnsep}{0.55cm} % Distance between the two columns of text
\setlength{\fboxrule}{0.75pt} % Width of the border around the abstract

%----------------------------------------------------------------------------------------
%	COLORS
%----------------------------------------------------------------------------------------

\definecolor{color1}{RGB}{0,0,180} % Color of the article title and sections
\definecolor{color2}{RGB}{0,20,20} % Color of the boxes behind the abstract and headings
\definecolor{color3}{RGB}{0,180,90} % Color of the boxes behind the abstract and headings

%----------------------------------------------------------------------------------------
%	HYPERLINKS
%----------------------------------------------------------------------------------------

\usepackage{hyperref} % Required for hyperlinks
\hypersetup{hidelinks,colorlinks=true,breaklinks=true,urlcolor=color3,citecolor=color1,linkcolor=color1,bookmarksopen=false,pdftitle={Title},pdfauthor={Author}}

%----------------------------------------------------------------------------------------
%	ARTICLE INFORMATION
%----------------------------------------------------------------------------------------

\JournalInfo{CS-E3210 Data Analysis Project} % Journal information
\Archive{Aalto University} % Additional notes (e.g. copyright, DOI, review/research article)

\PaperTitle{Comparison of learning models for music genre classification} % Article title

\Authors{Student\textsuperscript{1}, Student\textsuperscript{2}} % Authors
\affiliation{\textsuperscript{1}\textit{}} % Author affiliation
\affiliation{\textsuperscript{2}\textit{}} % Author affiliation
%\affiliation{\textbf{Corresponding author}: john@smith.com} % Corresponding author

\Keywords{Keyword1 --- Keyword2 --- Keyword3} % Keywords - if you don't want any simply remove all the text between the curly brackets
\newcommand{\keywordname}{Keywords} % Defines the keywords heading name

%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------

\Abstract{\lipsum[1]~}

%----------------------------------------------------------------------------------------

\begin{document}
	\boldmath

\flushbottom % Makes all text pages the same height

\maketitle % Print the title and abstract box

\tableofcontents % Print the contents section

\thispagestyle{empty} % Removes page numbering from the first page

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------
\phantomsection
\section{Introduction} % The \section*{} command stops section numbering

%\addcontentsline{toc}{section}{Introduction} % Adds this section to the table of contents

\lipsum[1-3] % Dummy text
 and some mathematics $\cos\pi=-1$ and $\alpha$ in the text\footnote{And some mathematics $\cos\pi=-1$ and $\alpha$ in the text.}.

%------------------------------------------------

\section{Data Analysis}

%\begin{figure*}[ht]\centering % Using \begin{figure*} makes the figure take up the entire width of the page
%\includegraphics[width=\linewidth]{view}       
%\caption{Wide Picture}
%\label{fig:view}
%\end{figure*}

\lipsum[4] % Dummy text

\begin{equation}
\cos^3 \theta =\frac{1}{4}\cos\theta+\frac{3}{4}\cos 3\theta
\label{eq:refname2}
\end{equation}

\lipsum[5] % Dummy text

\begin{enumerate}[noitemsep] % [noitemsep] removes whitespace between the items for a compact look
\item First item in a list
\item Second item in a list
\item Third item in a list
\end{enumerate}

\subsection{Subsection}

\lipsum[6] % Dummy text

\paragraph{Paragraph} \lipsum[7] % Dummy text
\paragraph{Paragraph} \lipsum[8] % Dummy text

\subsection{Subsection}

%\lipsum[9] % Dummy text

%\begin{figure}[ht]\centering
%\includegraphics[width=\linewidth]{results}
%\caption{In-text Picture}
%\label{fig:results}
%\end{figure}

Reference to Figure \ref{fig:results}.

\section{Methods and Experiments} % The \section*{} command stops section numbering

%\addcontentsline{toc}{section}{Methods and Experiments} % Adds this section to the table of contents
\underline{Fundamental intuition for choosing models:} Multi-class classification being the underlying aspect of the problem statement, we turned to learning models which constitute this feature.

\subsection{Logistic Regression}~\\
\underline{Motivation:} Logistic regression though by nature the default for binary classification, the fact that it can be extended for multi-class models was the motivation for choosing it.

Our model predicted the probabilities of different possible genres (labels) corresponding to a given feature space (see the data analysis section). For each of the multi-classification modelling techniques [one-vs-one - $ovo$, one-vs-all - $ovr$], we empirically used various algorithms as solvers (like stochastic average gradient) and consecutively varied the following parameters.
\begin{itemize}
	\item Maximum iterations for convergence ($100-900$)
	\item Initial class weights (balanced weighing, initializing to $1$)
	\item Weight regularization strength ($0.1-0.9$)
	\item Tolerance of error across runs 
\end{itemize}
in focus of improving on the accuracy of and reducing loss of the classification.

To optimize the hyper-parameters for modelling the logistic regression classifier we made use of the exhaustive Grid-search cross validation technique.  
 
\subsection{Support Vector Machines}~\\
\underline{Motivation:} As the feature-space is of a relatively higher dimensionality, we decided to try out support vector machines to improve on the statistics achieved using the logistic model.

Initially, we modelled a linear support vector classifier which used set of hyperplanes for learning the training data. As we did not observe any drastic improvements in the statistics, we further analysed the data and decided to experiment with the non-linear modelling of the support vector machines.

While empirically using both $ovo$ and the $ovr$ techniques, we tested various kernels:
\begin{itemize}
	\item Gaussian radial basis function
	\item Polynomial (degree 3)
	\item Sigmoid
\end{itemize}

For each of the kernels we varied the following parameters:
\begin{itemize}
	\item Maximum iterations for convergence ($100-300$)
	\item Initial class weights (balanced weighing, initializing to $1$)
	\item Weight regularization strength ($0.1-0.9$)
	\item Tolerance of error across runs 
\end{itemize}

To optimize the hyper-parameters for modelling the support vector classifier we made use of the exhaustive Grid-search cross validation technique.
\subsection{Ensemble Classifier}~\\
\underline{Motivation:} After experimenting with the above two models for sometime, we figured that logistic regression classifiers were good with accuracy metric and SVMs were minimizing on the log-loss statistic. So we decided to ensemble them together to understand if we can get the better of both worlds.

We used the \textbf{voting classifier} strategy to combine the above classifiers using a majority vote to predict the genres. We used both hard and the average predicted probabilities voting (soft vote) methods and found that the hard voting produced better results.

\subsection{Semi-Supervised Classifier}~\\

\underline{Motivation:} After not being able to further deduce any meaningful direct relationship between the feature-space, we decided to try the semi-supervised learning methodology.

We chose the label-spreading strategy over label propagation as it is more robust to noise{\cite{label-spreading}}. Modelling steps:
\begin{itemize}
	\item[\textit{Step 1}] Splitting the labelled (train) data as based on some percentage (we chose $\frac{1}{3}^{rd}$ of the it for validation)
	\item[\textit{Step 2}] Merging the split of labelled data from [step-1] with the unlabelled data.
	\item[\textit{Step 3}] Training the classifier/kernel chosen for label-spreading with respective parameters (listed below)
	\item[\textit{Step 4}] Scoring/validating the trained model using the split test data from [step-1]
	\item[\textit{Step 5}] Predicting the genres for unlabelled (actual test) data
\end{itemize}

We varied the following parameters over runs:
\begin{itemize}
	\item Kernels ($rbf$, k-nearest neighbours). Varying the gamma parameter (b/w $0.1-00001$) and the number of neighbours attribute (b/w $50-300$) respectively.
	\item Maximum iterations for convergence ($100-300$)
\end{itemize}

\subsection*{Other models}~\\
Apart from the above mentioned classifiers, we experimented with more models that did not pass our benchmarks for both the accuracy and loss metrics. 
\begin{itemize}
	\item Naive Bayes'
	\item Decision trees
	\item Random forests
\end{itemize}

Though we initially considered reducing the dimensionality by performing principal component analysis, after some substantial scrutiny of the feature space we dropped it. The rhythm bands, pitch classes and the timbre coefficients we spread out across the statistics to discard.

\subsection*{Performance Metrics}~\\
Evaluation of all the models designed was done using the $k$-fold cross validation technique (with $k=5$). 

For splitting the data into folds we used stratified process of preserving the percentage of samples for each genre thereby enabling fair evaluation.
%------------------------------------------------

%------------------------------------------------


\section{Results}

Performance measures of the above experiments when evaluated with the test data.

\subsection{Accuracy, Log-Loss statistics}
\begin{itemize}
	\item Logistic Regression Classifier across various values for its parameters. Results for the best 5 combinations:
	\begin{table}[H]
		\caption{Accuracy on test data (LR)}
		\centering
		\begin{tabular}{lllr}
			\toprule
%			\multicolumn{2}{c}{Name} \\
%			\cmidrule(r){1-2}
			modelling strategy & solver & weights & accuracy\\ 
			\midrule
			ovr & sag & None & $0.658$ \\
			multinomial & sag & None & $0.653$ \\
			ovr & liblinear & None & $0.650$ \\
			ovr & lbfgs & None & $0.646$ \\
			ovr & newton-cg & None & $0.646$ \\
			\bottomrule
		\end{tabular}
		\label{tab:label}
	\end{table}
	\item Support vector classifiers across various values for its parameters. Results for the best 5 combinations:
	\begin{table}[H]
		\caption{Accuracy on test data (SVC)}
		\centering
		\begin{tabular}{lllr}
			\toprule
			%			\multicolumn{2}{c}{Name} \\
			%			\cmidrule(r){1-2}
			modelling strategy & kernel & weights & accuracy\\ 
			\midrule
			multinomial & rbf & None & $0.636$ \\
			ovr & rbf & None & $0.636$ \\
			multinomial & linear & None & $0.607$ \\
			ovr & linear & None & $0.607$ \\
			ovr & poly (degree 3) & None & $0.567$ \\
			\bottomrule
		\end{tabular}
		\label{tab:label}
	\end{table}
	\item Ensemble classifier with best parametric combination of logistic regression, SVC and voting strategy produces an accuracy:
	$$=0.631$$
	\item Semi-supervised classifier with label spreading:
	\begin{table}[H]
		\caption{Accuracy on test data (semi-supervised)}
		\centering
		\begin{tabular}{llr}
			\toprule
			%			\multicolumn{2}{c}{Name} \\
			%			\cmidrule(r){1-2}
			kernel & attribute & accuracy\\ 
			\midrule
			knn & neighbours(300) & $0.533$ \\
			rbf & $\gamma=0.001$ & $0.612$ \\
			\bottomrule
		\end{tabular}
		\label{tab:label}
	\end{table}
\end{itemize}

\subsection{Confusion matrix}
\begin{figure}[H]
	\includegraphics[width=\linewidth]{confusion-matrix.jpg}
\end{figure}

\section{Discussion}

%----------------------------------------------------------------------------------------
\section{Appendices}

Thank you! Google!

%----------------------------------------------------------------------------------------
%	REFERENCE LIST
%----------------------------------------------------------------------------------------
\phantomsection
\begin{thebibliography}{1}
\bibitem{label-spreading} 
\href{http://scikit-learn.org/stable/modules/generated/sklearn.semi\_supervised.LabelSpreading.html}{semi-supervised learning,}
\end{thebibliography}

%----------------------------------------------------------------------------------------


\end{document}